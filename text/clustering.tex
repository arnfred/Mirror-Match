%\documentclass[12pt,journal,compsoc]{IEEEtran}
\documentclass{article}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\usepackage[stable]{footmisc}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}

\title{Reliable feature point matching without the use of geometric 
constraints}
\author{Jonas Arnfred}

\maketitle

\begin{abstract}
Many algorithms have been proposed to solve the problem of matching 
feature points in two or more images using geometric assumptions to 
increase the robustness of the matching. However very little work 
addresses the case where these assumptions might not hold true. In 
particular few methods address the problem of reliable matching in cases 
where we don't know if two images have any corresponding areas or 
objects. In this paper we propose two algorithms for matching feature 
points without the use of geometric constraints. The first makes use of 
the fact that any match between two images should be better than all 
possible matches within the image itself. The second algorithm extends 
this idea by using the community structure in the similarity graph of 
feature points to reliably find correspondences. To evaluate the 
algorithms experimentally we introduce an image set of 8 image pairs all 
containing view port changes and a simple method to generate a large 
amount of test cases based on these 8 pairs. The experimental results 
show that the proposed algorithm, overall, is superior to traditional 
approaches in finding correct correspondences.
\end{abstract}

\section{Introduction}

Since the introduction of feature points, several methods for matching 
them across images have been proposed in order to maximize the amount of 
correct matches. In this part of the project I introduce two methods to 
reliably match feature points. The word 'reliable' covers that matches 
only are returned if they are judged as sound. That is, matches should 
only be returned if there are objects or scenes that appear in both 
images.

When matching feature points between two images we are often faced with 
a dilemma between making the feature descriptor more discriminative to 
make sure only correct matches are made, and making it less 
discriminative to increase the robustness of the descriptor over 
different changes such as illumination, perspective and image quality.  
Any feature type aiming for robustness will have cases where it isn't 
possible to generate reliable matches if we only look at the pairwise 
correspondence of the descriptors. 

To get around this limitation several methods have been proposed in the 
literature. One of the most fruitful approaches have been to look at the 
geometric configuration of the feature points and match them with 
respect to assumptions concerning the variance between images. For 
example an angular and distance constraint between matches as considered 
by \cite{kim2008efficient} performs well in situations where no camera 
rotation occurs between the two images but will likely fail in other 
cases. Various scenarios have been proposed to improve on this simple 
assumption such as epipolar constraints (\cite{torr2000mlesac}, 
\cite{chum2005matching}) and pairwise constraints 
(\cite{choi2009robust}, \cite{leordeanu2005spectral}). The Epipolar 
constraint carries the assumption that the two images matched are 
related by an affine transformation. That is, there is no relative 
movement of objects in between images and either the viewpoint is fixed 
or the image resides entirely on a plane. In practice this assumption 
largely holds true when all objects we are interested in matching are 
roughly the same distance from the camera and when we don't expect to 
match objects that aren't consistently positioned across images.  
Pairwise constraints provide a more robust approach to this problem by 
looking at a set of proposed correspondences and defining a pairwise 
error between any two matches usually based on the assumption that two 
neighboring correspondences will usually have similar angles and 
distances. We can then convert the problem to an optimization problem 
and return a set of correspondences that minimizes this error such as 
proposed in \cite{choi2009robust} and \cite{leordeanu2005spectral}.  
This approach provides more robustness because assumptions that might 
not be true globally often still holds on a local level.

Another approach proposed in the literature is to pick out different 
zones in each image and find a pairing of zones where feature points are 
only matched within the same zone. \cite{das2008event} proposes 
clustering the feature points using their geometric position and 
matching only feature points between groups that have a certain amount 
of matchings between them. A more sophisticated approach is introduced 
in \cite{wu2011robust} where the zones are created by surrounding MSER 
feature points by an ellipse and grouping the SIFT or SURF feature 
points that fall within the ellipse together. These points can then be 
matched according to epipolar constraints.

In practice there are many situations where using the geometry of the 
image to filter correspondences is not possible. This situation most 
often arise when the assumptions made by a geometric method turns out 
not to hold in a particular use case. This could matchings between 
images where the content has been scrambled or with so little 
consistency that even the geometric assumptions break down even on a 
local level. In addition the geometric methods all require a set of 
correspondences to begin with. If this set of correspondences is 
narrowed down to a smaller set with a higher ratio of inliers the result 
of the geometric matching will be both faster and more accurate. Finally 
some use cases might require a performance that can't be achieved by a 
more complex geometric method where simple non geometric methods might 
be able to provide additional matching accuracy with a smaller 
performance penalty. This approach is inspired by \cite{wu2009bundling} 
where bundling of feature points was used to enhance the search results 
in large scale partial-duplicate web image search.

There are a relatively small number of algorithms proposed to solve the 
matching problem without involving geometric constraints. Traditionally 
the feature points of two images have been matched by comparing the 
every feature point of one image with all feature points of the other 
and finding the best matches based on the similarity of the descriptors.  
With the introduction of the SIFT features \cite{lowe2004sift}, Lowe 
proposes an alternative measure where the uniqueness of a given match is 
found by looking at the two nearest neighbors of each feature point and 
calculating a match score by the ratio of similarities. By ranking the 
scores by their uniqueness and picking the $n$ best we get a set of 
correspondences that are distinctly matched across the two images. In 
the book Multiple View Geometry \cite[p.  114]{multipleView}, the gold 
standard algorithm is introduced which ranks correspondences by 
similarity and further filters them by discarding matches that aren't 
symmetric, i.e. that for every feature point $f_i$ matching $f_j$ we 
also have that $f_j$ matches $f_i$.

In turn the two methods proposed in this report are inspired by a simple 
but novel idea. If we have two images and a given feature point in the 
first image is better matched with other feature points from the 
\emph{same} image than points in the other image, then any matches of 
this feature point to points in the other image is deemed unreliable and 
discarded.  This approach carries no implicit assumptions about the 
geometric consistency of matches and can as such easily be extended with 
other geometric solutions when appropriate.

Based on this idea the two proposed methods find reliable matches as 
follows:
\begin{itemize}
\item{\emph{Mirror Match (MM)}: Match features using the gold standard 
algorithm\cite[p. 114]{multipleView} ranked by similarity and thresheld 
by uniqueness\cite{lowe2004sift}. However instead of matching features 
from one image with features in another, we match every feature with all 
other features of the two images combined. Only matches from one image 
to the other are returned.}
\item{\emph{Mirror Match with Clustering (MMC)}: Take the combined set 
of feature points from both images and cluster these points according to 
their descriptors. Given a resulting partition of points, no matches are 
returned if it contains only feature points from one image. If the 
partition contains points from both images, \emph{Mirror Match} is used 
to find the best matches within the partition.}
\end{itemize}

Matching feature points against the entire set of points from both 
images ensures that the distinctiveness of a returned correspondence is 
higher. In almost all cases\footnote{Exceptions would include cases 
where we want to find all particular points in a pattern and other use 
cases where the correspondence isn't assumed to be unique}, a good match 
between two images is unambiguous in the sense that there are no other 
equally (or almost equally) good potential matches to the same point.  
This is the key insight behind the algorithm presented by Lowe in 
\cite{lowe2004sift}. However the implication doesn't follow the other 
way around. In the case that a feature point don't have any actual 
correspondence in the other image, there is no guarantee that this 
feature point might not still be uniquely matched to a feature point in 
the other image. The issue is particularly pronounced if we compare two 
images that don't correspond. For any proposed correspondence it is 
entirely probable that this correspondence is unique even if it isn't 
correct. With Mirror Match this ambiguity is avoided by incorporating 
the feature points of both images when a match is made. When the two 
images have nothing to do with each other, chances are that a feature 
point will match better with another feature point from the same image 
in which case it is easily discarded.

Often images will contain repetitive patterns that are difficult to 
match\footnote{In fact this particular problem has been given attention 
before by for example \cite{fan2011towards}} because the feature points 
covering these patterns will look very similar. If we threshold by 
uniqueness like in Mirror Match this means that these points will often 
be discarded even if they might have good correspondences. To solve this 
problem the enhanced Mirror Match with Clustering makes it possible to 
look at groups of similar feature points one at a time and within each 
group finding the best matches. This is done by taking the set of 
feature points from both images and clustering. The resulting partitions 
will each contain feature points that are similar to each other. In some 
cases the clustering will end up grouping feature points of only one 
image together which can then easily be discarded in the matching 
process. In other cases partitions will contain feature points from both 
images that can then be matched using \emph{MM} but with lower 
thresholds.

The report is organized as follows. In section \ref{algorithms} I will 
describe the two algorithms. Section \ref{experiment} will cover the 
experimental set up and introduce the image set that is used in testing.  
In section \ref{results} I will go through the results and conclude.

\section{Algorithms}
\label{algorithms}

To discuss the implementation of \emph{MM} and \emph{MMC} a few notes on 
notation will be helpful. BLAH BLAH!

\subsection{Mirror Match (\emph{MM})}

The central idea behind \emph{MM} is to match features of $n$ images by 
taking every feature from all $n$ images and matching it against every 
other feature from the same set. We can then discard the correspondences 
that match two points within the same image. In algorithm \ref{alg-mm} 
the actual implementation of \emph{MM} is shown.

\begin{algorithm}
\caption{Mirror Match Algorithm (\emph{MM})}
\label{alg-mm}
\begin{algorithmic}
\Require $images$ : set of images, $threshold$ : float $\in \mathbb{R}$
\State $matches_{init}\gets \varnothing$
\State $matches_{final}\gets \varnothing$
\State $features\gets \varnothing$
\ForAll{$I_i \in images$} \Comment Gather features of all images
	\State $features\gets features \cup getFeatures(I_i)$
\EndFor
\ForAll{$f_i \in features$} \Comment Get best match for all features
	\State $f_m,f_n \gets getTwoNearestNeighbors(f_i, features ~ 
\backslash ~ \left\{f_i\right\})$
	\State $ratio \gets getSimilarity(f_i, f_n) / getSimilarity(f_i, 
f_m)$
	\If{$ratio < threshold$}
		\State $matches_{init} \gets \left(f_i, f_m\right)$
	\EndIf
\EndFor
\ForAll{$\left(f_i, f_j \right) \in matches_{init}$} \Comment Filter 
matches
\If{$\left(f_j, f_i \right) \in matches_{init} \wedge getImg(f_i) \neq 
getImg(f_j) \wedge \left(f_j, f_i\right) \not\in matches_{final}$}
		\State $matches_{final} \gets (f_i, f_j)$
	\EndIf
\EndFor \\
\Return $matches_{final}$
\end{algorithmic}
\end{algorithm}

This algorithm passes over three states: An acquisition state where 
feature points are gathered, a matching state where the initial set of 
correspondences are found and a filtering state where correspondences 
within the same image are removed. In the acquisition state the function 
\emph{getFeatures($I_i$)} returns a set of feature points given an 
image.  In practice the feature used for the experiments have been 
SIFT\cite{lowe2004sift}, but there aren't any reason why other features 
such as SURF\cite{bay2006surf} or BRIEF\cite{calonder2010brief} couldn't 
be used just as well. The matching state uses K-Nearest Neighbors to 
return the two closest neighbors in the set of features for any given 
feature $f_i$ and calculates the ratio between the correspondences as 
proposed in \cite{lowe2004sift}.  Any correspondence with a ratio below 
the threshold supplied will be discarded. Finally in the filter the 
function \emph{getImg($f_i$)} returns the parent image of the feature 
$f_i$ and discards both correspondences that aren't symmetric as well as 
those that match two points in the same image.

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/mirror_match_off}
			\caption{Baseline Result}
			\label{fig:unique}
		\end{subfigure}%
		%~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/mirror_match_with_pruned}
			\caption{\emph{MM} with intra image matches}
			\label{fig:within}
		\end{subfigure}%
		%~ %add desired spacing between images, e. g. ~, \quad, \qquad
		  %(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.45\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/mirror_match}
			\caption{\emph{MM}}
			\label{fig:without}
		\end{subfigure}%
	}%
	\label{fig:compare_mirror}
	\caption{Illustration of matches filtered by \emph{MM}}
\end{figure}

Figure \ref{fig:compare_mirror} shows an example of the performance of 
\emph{MM}. In sub figure \ref{fig:unique} the result of a normal 
matching of two images is shown. The two images appear side by side and 
for each correspondence between them, a line has been drawn from one 
image to the other. The color of the line indicates if the 
correspondence is accurate. Sub figure \ref{fig:within} shows the 
correspondences found by mirror match using the same threshold before 
the matches within the same image has been filtered out. In particular 
the fence by the bottom of the image admits correspondences that can 
conveniently be removed as done in sub figure \ref{fig:without} where 
only the actual correspondences returned by the algorithm are shown.

\subsection{Mirror Match with Clustering (\emph{MMC})}

As opposed to \emph{MM}, \emph{MMC} diverges from traditional 
non-geometric feature matching by clustering the feature points by 
similarity. This process yields partitions of fairly similar feature 
points that we can match using the same approach as \emph{MM}. Before I 
introduce the implementation details I will go over the problem of graph 
clustering and how it relates to feature matching.

\subsubsection{Graph Clustering}

Given a set of feature points from two images, $im_1$ and $im_2$: $F_k = 
\{k_i, k_j$ for $k_i \in im_1, k_j \in im_2\}$ as well as a matching 
function $M(k_i, k_j) \rightarrow \mathbb{R}$ that takes two feature 
points in an image and return their matching score, we can define a 
matrix $A$ where each element $A_ij = M(k_i, k_j)$. A can be interpreted 
as the \emph{adjecency matrix} of the fully connected graph where each 
vertex corresponds to a keypoint and the edge between two vertices has a 
weight equal to the distance between the two corresponding keypoints.

This representation reduces the problem of partitioning the keypoints 
into groups to that of graph clustering or community structure depending 
on the context. In the literature there are various ways of clustering a 
graph according to different measures of what constitutes an optimal 
partitioning. Traditionally the most used clustering algorithms have 
been K-means and spectral clustering, but in recent years a host of new 
algorithms have been proposed based on both Newman's concept of graph 
modularity\footnote{Introduced in \cite{girvan2002}, discussed in 
\cite{brandes2007} and used in \cite{blondel2008} as well as others} as 
well as information theoretical measures\footnote{See for example 
\cite{rosvall2008}} and the Potts spin model from physics\footnote{Used 
in \cite{ronhovde2009}} just to mention a few approaches. Many of the 
new algorithms differ from K-means clustering and Spectral clustering in 
that they don't require the number of expected to clusters to be 
specified beforehand\footnote{Among the aforementioned methods, this is 
true for \cite{blondel2008} and \cite{rosvall2008}}.  Furthermore, on 
tests done using randomly generated graphs with a known partitioning 
\cite{blondel2008}, \cite{rosvall2008} and \cite{ronhovde2009} perform 
markedly better than spectral clustering and 
K-means\cite{lancichinetti2009}.

The performance of clustering algorithms is a complicated issue, since 
an optimal clustering given the same graph can vary depending on the 
application. Spectral clustering for example will usually return a 
partitioning where each partition is roughly equal in size\footnote{As 
mentioned in \cite{von2007}} while the Louvain 
clustering\cite{blondel2008} might return partitions of very uneven 
size, even if the modularity measurement has been shown to penalize very 
small clusters\cite{brandes2007}. Both behaviours can be beneficial 
depending on the application, but when clustering feature points, 
maintaining clusters of an even size usually means that some clusters 
will be '\emph{catch-all}' clusters where the feature points that don't 
fit anywhere else are grouped together. The necessity of specifying the 
amount of partitions in for example Spectral clustering or Pott's model 
clustering further exacerbates the issue since smaller partitions are 
then combined into one to achieve the right amount of partitions.

Based on these observations I've decided to use Louvain clustering for 
grouping feature points since it's fast, performs well, doesn't require 
parameters and doesn't emphasize partitions of equal sizes.

\subsubsection{Clustering feature points using Louvain clustering}

\begin{algorithm}
\caption{Mirror Match with Clustering Algorithm (\emph{MMC})}
\label{alg-mmc}
\begin{algorithmic}
\Require $images$ : set of images, $threshold \in \mathbb{R}$
\State $matches\gets \varnothing$
\State $features\gets \varnothing$
\ForAll{$I_i \in images$} \Comment Gather features of all images
	\State $f_i\gets getFeatures(I_i)$
	\State $features\gets features \cup f_i$
\EndFor
\State $A\gets getAdjecencyMatrix(f_1, f_2,\; \ldots \;, f_n)$
\State $A_{norm}\gets 1 - normalize(A)$
\Comment Shorter distance is higher weight
\State $A_{pruned}\gets pruneEdges(A_{norm},\alpha)$
\State $Partitions\gets cluster(A_{pruned})$
\ForAll{$p \in Partitions$} \Comment p is a set of feature points
	\State $matches\gets matches \cup getMatches(p, threshold, 
features)$
\EndFor \\
\Return matches
\end{algorithmic}
\end{algorithm}

Unlike the \emph{MM} algorithm the feature points are partitioned before 
the matching stage. While the louvain clustering algorithm doesn't 
require any parameters in itself, it tends towards clustering all 
feature points together in the same partition if the graph is fully 
connected or close. To ensure that the graph is well clustered, the 
adjacency matrix is pruned so only edges above a certain threshold is 
kept. From empirical analysis keeping around 2.5\% of edges seems to 
work well. Figure \ref{fig:graph} shows the result of clustering the 
feature points displayed as a graph where each node correspond to a 
feature point. Here the border of nodes signifies the original image 
while the color of the node is decided by what partition it belongs to.  
Notice that since a limited amount of colors are used disconnected 
subgraphs of the same color can be assumed to belong to separate 
partitions. The sub figure \ref{fig:cropped_graph} shows a detailed 
section of the graph where the interplay between small partitions 
consisting of two or three nodes as well as larger partitions consisting 
of several hundred nodes can be seen.

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.60\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/MMC_graph_full}
			\caption{Full Graph}
			\label{fig:full_graph}
		\end{subfigure}%
		%~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.60\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/MMC_graph_cropped}
			\caption{Cropped Section of Graph}
			\label{fig:cropped_graph}
		\end{subfigure}%
	}%
	\label{fig:graph}
	\caption{The partitioned feature graph. Every color signifies a 
partition while the edge color of each node signifies which image it 
belongs to}
\end{figure}

The partitions group feature points by similarity which means that 
repetitive structures such as buildings often feature in larger 
partitions as exemplified in figure \ref{fig:compare_mirror}. Here the 
cluster shown in part \ref{fig:pitts_partition} contains left window 
corners from two buildings across both images. This is an example of 
feature points that are hard to match correctly since each point will 
have several similar potential matches.

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.60\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/MMC_pitts_source}
			\caption{Source image}
			\label{fig:pitts_source}
		\end{subfigure}%
		%~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.60\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/MMC_pitts_keypoints}
			\caption{Keypoints Displayed}
			\label{fig:pitts_keypoints}
		\end{subfigure}%
	}%
	\\
	\makebox[\textwidth][c]{
		%~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		
		\begin{subfigure}[t]{1.16\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/MMC_partition}
			\caption{A Partition Example}
			\label{fig:pitts_partition}
		\end{subfigure}%
	}%
	\label{fig:compare_mirror}
	\caption{Example of a partition of keypoints in an image}
\end{figure}

The matching algorithm for \emph{MMC} (algorithm \ref{alg-getmatches}) 
goes through the partitions one by one and selects matches in the 
following way: If the partition has no edges going from a feature point 
in one image to a feature point in another image, the partition is 
discarded. If we have more than one edge going between images then 
$getMatches$ will behave as \emph{MM} and make sure that a 
correspondence is symmetric, that it has a ratio above the desired 
threshold and that it isn't matching two feature points from the same 
image. Since feature points are all guaranteed to be fairly similar this 
means that we can set the ratio threshold much higher than in \emph{MM} 
and include matches that would normally have been discarded for not 
being sufficiently unique.
However as can be seen in the example in figure \ref{fig:cropped_graph} 
many of the partitions contain only two feature points from different 
images linked by one edge. This is often the case if all edges below a 
certain similarity threshold have been removed in the graph, in which 
case it's probable that some nodes will end up with only one outgoing 
edge. This doesn't necessarily imply that the correspondence represented 
by the edge is unique, but the only way to figure out if it is, is by 
checking the similarity scores of the nearest neighbor as proposed in 
\cite{lowe2004sift}.

\begin{algorithm}
\caption{Impl. of getMatches (\emph{from MMC algorithm})}
\label{alg-getmatches}
\begin{algorithmic}
\Require $p$ : set of features, $threshold \in \mathbb{R}$, $features$ : 
Set of all features
\State $matches \gets \varnothing$
\State $edges \gets \left\{getSimilarity(f_i, f_j) \mid getImg(f_i) \neq 
getImg(f_j) \wedge f_i, f_j \in p \right\}$
\If{$\left\vert edges \right\vert = 1$} \Comment If $\exists$ one edge 
between images
	\State $f_i,f_j \gets getInterImageFeatures(edges_{inter}, p)$
	\State $f_m,f_n \gets getTwoNearestNeighbors(f_i, features ~ 
\backslash ~ \left\{f_i\right\})$
	\State $f_s,f_t \gets getTwoNearestNeighbors(f_j, features ~ 
\backslash ~ \left\{f_j\right\})$
	\State $ratio_i \gets getSimilarity(f_i, f_n) / getSimilarity(f_i, 
f_j)$
	\State $ratio_j \gets getSimilarity(f_j, f_t) / getSimilarity(f_j, 
f_i)$
	\If{$ratio_i < threshold \wedge ratio_j < threshold$}
		\State $matches \gets matches \cup (f_i, f_j)$
	\EndIf
\ElsIf{$\left\vert edges \right\vert > 1$} \Comment If $\exists$ more 
than one edge between images
	\ForAll{$f_i \in p$}
		\State $f_m,f_n \gets getTwoNearestNeighbors(f_i, p ~ \backslash 
~ \left\{f_i\right\})$
		\State $ratio \gets getSimilarity(f_i, f_n) / getSimilarity(f_i, 
f_m)$
		\If{$getImg(f_i) \neq getImg(f_m) \wedge ratio < threshold 
\wedge (f_m, f_i) \not\in matches$}
			\State $matches \gets matches \cup (f_i, f_m)$
		\EndIf
	\EndFor
\EndIf

\Return matches
\end{algorithmic}
\end{algorithm}


The two algorithms introduced in this report are designed to work with 
feature points such as ,  or similar. While the experimental results 
have all been done with the SIFT descriptor, there aren't any 
theoretical or practical issues with using other types of feature 
points. 

In this report I propose two algorithms for better matching feature 
points without any geometric assumptions about the images matched. The 
two algorithms are complementary in the sense that the first, Mirror 
Match (\emph{MM}) is less complex but performs slightly worse than 
Mirror Match with Clustering (\emph{MMC}). In cases where high 


\section{Experiments}
\label{experiment}

To reliably measure the accuracity of a matching method on real images 
we either need a set of image pairs tied by a homography or manually 
count the number of inliers. The latter poses becomes unfeasable the 
moment we attempt to test a non-trivial set of images, but has the 
advantage of working with any image pair. Using a homography between 
image pairs reduces the test cases to images where the matched areas lie 
on a plane, or alternatively that there is no viewpoint change. This 
greatly reduces the body of image pairs we can use to reliably test 
matching methods. In \cite{mikolaczyk2005performance} Mikolajczyk and 
Schmid introduces a set of images to compare the performance of feature 
detectors. The test set looks at different feature descriptors 
invariance faced with different lighting, blur, rotation as well as 
viewpoint change. However in practice the image used for the view point 
change featuring a wall of graffiti\footnote{To ensure that all points 
lie on the same plane} has gone on to be used the most for later tests 
given that most of the other test pairs have since become relatively 
trivial\footnote{see \cite{wu2011robust} or \cite{delponte2006svd} for 
examples}.

Inspired by this I have collected a set of image pairs featuring murals 
of a few famous artists\footnote{Including Banksy, Blu and Shepard 
Fairey among others}. Each pair consist of images of the same motif 
taken from different angles, often including repetitive patterns and 
texture. Based on this image set the two algorithms are tested on a 
series of image patches from each pair. In order to test if a matching 
is reliable we of course need correct matches when the image pair we are 
testing on indeed features the same scene, but we also need to make sure 
that we don't get any matches when the two images have nothing in 
common. This means that it isn't enough just testing against pairs of 
images that we know match. We also need to test against pairs that might 
look like they could match but in fact don't.

With this in mind I have created a test suite by cropping out $300$ 
pixel by $300$ pixel patches of the original pair with a random vertical 
and horizontal offset. Given an image pair I can obtain $n$ pairs of 
patches where the two patches in any pair might or might not overlap.  
This ensures that the patch pairs where there is no overlap still retain 
a basic similarity to each other, while the patches that do overlap 
often only overlap on a small part of their surface. Figure 
\ref{fig:fairey} shows an example of possible pairs of test images 
(\ref{fig:fairey_crop}) produced from two source images 
(\ref{fig:fairey1} and \ref{fig:fairey2}). Producing $n$ such pairs 
allows us to test not just how well the matching algorithm performs on 
corresponding images, but also how many false positives we get on 
similar images that don't overlap.

The set of source images and homographies as well as the a script to 
generate the cropped test sets based on them have been published. The 
actual testsets used for testing in this paper are also available 
online\footnote{They can be found on INSERT URL HERE}.

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.50\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/pair_example1}
			\caption{Mural by Fairey (1)}
			\label{fig:fairey1}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.50\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/pair_example2}
			\caption{Mural by Fairey (2)}
			\label{fig:fairey2}
		\end{subfigure}%
	}%
	\\
	\makebox[\textwidth][c]{
		%~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		
		\begin{subfigure}[t]{1.16\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/crop_examples}
			\caption{Image pairs}
			\label{fig:fairey_crop}
		\end{subfigure}%
	}%
	\label{fig:fairey}
	\caption{Example of test set produced by pair of images}
\end{figure}

\subsection{Experimental setup}

Using the image test sets described above, the \emph{MM} and \emph{MMC} 
algorithms have been tested against a baseline algorithm incorporating 
ideas from the algorithm Lowe proposed in \cite{lowe2004sift} as well as 
the gold standard algorithm for calculating homographies proposed in 
\cite{multipleView}. For reference the algorithm can be seen in 
\ref{alg:baseline}.

\begin{algorithm}
\caption{Baseline Match Algorithm (\emph{BM})}
\label{alg:baseline}
\begin{algorithmic}
\Require $image_1$ : image, $image_2$ : image, $threshold$ : float $\in 
\mathbb{R}$
\State $matches_{init}\gets \varnothing$
\State $matches_{final}\gets \varnothing$
\State $features_1 \gets getFeatures(image_1)$
\State $features_2 \gets getFeatures(image_2)$
\ForAll{$f_i \in features_1$} \Comment Get best match for all features 
in $image_1$
	\State $f_m,f_n \gets getTwoNearestNeighbors(f_i, features_2)$
	\State $ratio \gets getSimilarity(f_i, f_n) / getSimilarity(f_i, 
	f_m)$
	\If{$ratio < threshold$}
		\State $matches_{init} \gets \left(f_i, f_m\right)$
	\EndIf
\EndFor
%
\ForAll{$f_i \in features_2$} \Comment Get best match for all features 
in $image_2$
	\State $f_m,f_n \gets getTwoNearestNeighbors(f_i, features_1)$
	\State $ratio \gets getSimilarity(f_i, f_n) / getSimilarity(f_i, 
	f_m)$
	\If{$ratio < threshold$}
		\State $matches_{init} \gets \left(f_i, f_m\right)$
	\EndIf
\EndFor
%
\ForAll{$\left(f_i, f_j \right) \in matches_{init}$} \Comment Filter 
matches
\If{$\left(f_j, f_i \right) \in matches_{init} \wedge \left(f_j, 
f_i\right) \not\in matches_{final}$}
		\State $matches_{final} \gets (f_i, f_j)$
	\EndIf
\EndFor \\
\Return $matches_{final}$
\end{algorithmic}
\end{algorithm}

For each test set of cropped image pairs the three 
algorithms\footnote{\emph{MM}, \emph{MMC} and \emph{BM}} are applied to 
each pair of images, and the threshold for each is varied over 50 evenly 
spaced values. For \emph{MM} and \emph{BM} these values lie between 
$0.4$ and $0.8$ where as the threshold for \emph{MMC} is varied between 
$0.8$ and $1.0$. For each match returned by any of the algorithms, the 
match is registered as an inlier if the endpoint in the second image 
lies within $5$px of the true position calculated by the homography.  
That is, given $H$ relating $I_1$ with $I_2$ and a match ($p_1, p_2$) 
then the match is an inlier if $\left\vert Hp_1 - p_2 \right\vert < 5$.  
This is a relatively lenient threshold since radial distortion from the 
lens as well as inconsistencies to the planar assumption of the 
background means that $H$ can not be entirely accurate. The experiments 
are run on the $8$ image pairs in the test set, as well as the graf 
image from \cite{mikolajczyk2005performance}.

\section{Results}
\label{results}

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/fairey_burma_1}
			\caption{Burma by Fairey (1)}
			\label{fig:burma1}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/fairey_burma_2}
			\caption{Burma by Fairey (2)}
			\label{fig:burma2}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/result_fairey_burma}
			\caption{X: Nb of correct matches. Y: Accuracity}
			\label{fig:result_burma}
		\end{subfigure}%
	}%
	\label{fig:burma}
	\caption{results on Burma by Fairey}
\end{figure}

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.25\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/fairey_lady_1}
			\caption{Lady by Fairey (1)}
			\label{fig:lady1}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.25\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/fairey_lady_2}
			\caption{Lady by Fairey (2)}
			\label{fig:lady2}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.55\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/result_fairey_lady}
			\caption{X: Nb of correct matches. Y: Accuracity}
			\label{fig:result_lady}
		\end{subfigure}%
	}%
	\label{fig:lady}
	\caption{results on Lady by Fairey}
\end{figure}


\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/scharf_1}
			\caption{Faces by Scharf (1)}
			\label{fig:faces1}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/scharf_2}
			\caption{Faces by Scharf (2)}
			\label{fig:faces2}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/result_scharf}
			\caption{X: Nb of correct matches. Y: Accuracity}
			\label{fig:result_faces}
		\end{subfigure}%
	}%
	\label{fig:faces}
	\caption{results on Faces by Scharf}
\end{figure}

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/houston_1}
			\caption{Houston (1)}
			\label{fig:houston1}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/houston_2}
			\caption{Houston (2)}
			\label{fig:houston2}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/result_houston}
			\caption{X: Nb of correct matches. Y: Accuracity}
			\label{fig:result_houston}
		\end{subfigure}%
	}%
	\label{fig:houston}
	\caption{Results on Houston}
\end{figure}

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.25\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/blu_head_1}
			\caption{head by blu (1)}
			\label{fig:head1}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.25\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/blu_head_2}
			\caption{head by blu (2)}
			\label{fig:head2}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.55\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/result_blu_head}
			\caption{X: Nb of correct matches. Y: Accuracity}
			\label{fig:result_head}
		\end{subfigure}%
	}%
	\label{fig:head}
	\caption{results on head by blu}
\end{figure}

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.25\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/blu_pencil_1}
			\caption{Pencil by Blu (1)}
			\label{fig:pencil1}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.25\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/blu_pencil_2}
			\caption{Pencil by Blu (2)}
			\label{fig:pencil2}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.55\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/result_blu_pencil}
			\caption{X: Nb of correct matches. Y: Accuracity}
			\label{fig:result_pencil}
		\end{subfigure}%
	}%
	\label{fig:pencil}
	\caption{Results on Pencil by Blu}
\end{figure}

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/banksy_city_1}
			\caption{City by Banksy (1)}
			\label{fig:city1}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/banksy_city_2}
			\caption{City by Banksy (2)}
			\label{fig:city2}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/result_banksy_city}
			\caption{X: Nb of correct matches. Y: Accuracity}
			\label{fig:result_city}
		\end{subfigure}%
	}%
	\label{fig:city}
	\caption{results on City}
\end{figure}

\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/banksy_stroller_1}
			\caption{Stroller by Banksy (1)}
			\label{fig:stroller1}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/banksy_stroller_2}
			\caption{Stroller by Banksy (2)}
			\label{fig:stroller2}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/result_banksy_stroller}
			\caption{X: Nb of correct matches. Y: Accuracity}
			\label{fig:result_stroller}
		\end{subfigure}%
	}%
	\label{fig:stroller}
	\caption{Results on Stroller by Banksy}
\end{figure}


\begin{figure}
	\makebox[\textwidth][c]{%
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/graf_1}
			\caption{Graf (1)}
			\label{fig:graf1}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/graf_3}
			\caption{Graf (3)}
			\label{fig:graf3}
		\end{subfigure}%
		~ %add desired spacing between images, e. g. ~, \quad, \qquad		  
		%(or a blank line to force the subfigure onto a new line)
		\begin{subfigure}[t]{0.35\textwidth}
			\centering
			\includegraphics[width=\textwidth]{images/result_graf}
			\caption{X: Nb of correct matches. Y: Accuracity}
			\label{fig:result_graf}
		\end{subfigure}%
	}%
	\label{fig:graf}
	\caption{results on Graf}
\end{figure}

In figure \ref{fig:stroller} to \ref{fig:graf} the results on the images

\bibliographystyle{IEEEtran}
\bibliography{bibliography}
\end{document}

