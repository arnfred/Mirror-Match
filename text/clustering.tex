\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[stable]{footmisc}
\usepackage{caption}
\usepackage{adjustbox}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}

\title{Clustering feature points}
\author{Jonas Arnfred}

\maketitle

\begin{abstract}
% TODO!
\end{abstract}

\section{Introduction}

Since the introduction of feature points, several methods for matching 
them across images have been proposed in order to maximize the amount of 
correct matches. In this part of the project I introduce two methods to 
reliably match feature points. The word 'reliable' covers that matches 
only are returned if they are judged as sound. That is, matches should 
only be returned if there are objects or scenes that appear in both 
images.

When matching feature points between two images we are often faced with 
a dilemma between making the feature descriptor more discriminative to 
make sure only correct matches are made, and making it less 
discriminative to increase the robustness of the descriptor over 
different changes such as illumination, perspective and image quality.  
Any feature type aiming for robustness will have cases where it isn't 
possible to generate reliable matches if we only look at the pairwise 
correspondence of the descriptors. 

To get around this limitation several methods have been proposed in the 
literature. One of the most fruitful approaches have been to look at the 
geometric configuration of the feature points and match them with 
respect to assumptions concerning the variance between images. For 
example an angular and distance constraint between matches as considered 
by \cite{kim2008efficient} performs well in situations where no camera 
rotation occurs between the two images but will likely fail in other 
cases. Various scenarios have been proposed to improve on this simple 
assumption such as epipolar constraints (\cite{torr2000mlesac}, 
\cite{chum2005matching}) and pairwise constraints 
(\cite{choi2009robust}, \cite{leordeanu2005spectral}). The Epipolar 
constraint carries the assumption that the two images matched are 
related by an affine transformation. That is, there is no relative 
movement of objects in between images and either the viewpoint is fixed 
or the image resides entirely on a plane. In practice this assumption 
largely holds true when all objects we are interested in matching are 
roughly the same distance from the camera and when we don't expect to 
match objects that aren't consistently positioned across images.  
Pairwise constraints provide a more robust approach to this problem by 
looking at a set of proposed correspondences and defining a pairwise 
error between any two matches usually based on the assumption that two 
neighboring correspondences will usually have similar angles and 
distances. We can then convert the problem to an optimization problem 
and return a set of correspondences that minimizes this error such as 
proposed in \cite{choi2009robust} and \cite{leordeanu2005spectral}.  
This approach provides more robustness because assumptions that might 
not be true globally often still holds on a local level.

Another approach proposed in the literature is to pick out different 
zones in each image and find a pairing of zones where feature points are 
only matched within the same zone. \cite{das2008event} proposes 
clustering the feature points using their geometric position and 
matching only feature points between groups that have a certain amount 
of matchings between them. A more sophisticated approach is introduced 
in \cite{wu2011robust} where the zones are created by surrounding MSER 
feature points by an ellipse and grouping the SIFT or SURF feature 
points that fall within the ellipse together. These points can then be 
matched according to epipolar constraints.

In practice there are many situations where using the geometry of the 
image to filter correspondences is not possible. This situation most 
often arise when the assumptions made by a geometric method turns out 
not to hold in a particular use case. This could matchings between 
images where the content has been scrambled or with so little 
consistency that even the geometric assumptions break down even on a 
local level. In addition the geometric methods all require a set of 
correspondences to begin with. If this set of correspondences is 
narrowed down to a smaller set with a higher ratio of inliers the result 
of the geometric matching will be both faster and more accurate. Finally 
some use cases might require a performance that can't be achieved by a 
more complex geometric method where simple non geometric methods might 
be able to provide additional matching accuracy with a smaller 
performance penalty. This approach is inspired by \cite{wu2009bundling} 
where bundling of feature points was used to enhance the search results 
in large scale partial-duplicate web image search.

There are a relatively small number of algorithms proposed to solve the 
matching problem without involving geometric constraints. Traditionally 
the feature points of two images have been matched by comparing the 
every feature point of one image with all feature points of the other 
and finding the best matches based on the similarity of the descriptors.  
With the introduction of the SIFT features \cite{lowe2004sift}, Lowe 
proposes an alternative measure where the uniqueness of a given match is 
found by looking at the two nearest neighbors of each feature point and 
calculating a match score by the ratio of similarities. By ranking the 
scores by their uniqueness and picking the $n$ best we get a set of 
correspondences that are distinctly matched across the two images. In 
the book Multiple View Geometry \cite[p.  114]{multipleView}, the gold 
standard algorithm is introduced which ranks correspondences by 
similarity and further filters them by discarding matches that aren't 
symmetric, i.e. that for every feature point $f_i$ matching $f_j$ we 
also have that $f_j$ matches $f_i$.

In turn the two methods proposed in this report are inspired by a simple 
but novel idea. If we have two images and a given feature point in the 
first image is better matched with other feature points from the 
\emph{same} image than points in the other image, then any matches of 
this feature point to points in the other image is deemed unreliable and 
discarded.  This approach carries no implicit assumptions about the 
geometric consistency of matches and can as such easily be extended with 
other geometric solutions when appropriate.

Based on this idea the two proposed methods find reliable matches as 
follows:
\begin{itemize}
\item{\emph{Mirror Match (MM)}: Match features using the gold standard 
algorithm\cite[p. 114]{multipleView} ranked by similarity and thresheld 
by uniqueness\cite{lowe2004sift}. However instead of matching features 
from one image with features in another, we match every feature with all 
other features of the two images combined. Only matches from one image 
to the other are returned.}
\item{\emph{Mirror Match with Clustering (MMC)}: Take the combined set 
of feature points from both images and cluster these points according to 
their descriptors. Given a resulting partition of points, no matches are 
returned if it contains only feature points from one image. If the 
partition contains points from both images, \emph{Mirror Match} is used 
to find the best matches within the partition.}
\end{itemize}

Matching feature points against the entire set of points from both 
images ensures that the distinctiveness of a returned correspondence is 
higher. In almost all cases\footnote{Exceptions would include cases 
where we want to find all particular points in a pattern and other use 
cases where the correspondence isn't assumed to be unique}, a good match 
between two images is unambiguous in the sense that there are no other 
equally (or almost equally) good potential matches to the same point.  
This is the key insight behind the algorithm presented by Lowe in 
\cite{lowe2004sift}. However the implication doesn't follow the other 
way around. In the case that a feature point don't have any actual 
correspondence in the other image, there is no guarantee that this 
feature point might not still be uniquely matched to a feature point in 
the other image. The issue is particularly pronounced if we compare two 
images that don't correspond. For any proposed correspondence it is 
entirely probable that this correspondence is unique even if it isn't 
correct. With Mirror Match this ambiguity is avoided by incorporating 
the feature points of both images when a match is made. When the two 
images have nothing to do with each other, chances are that a feature 
point will match better with another feature point from the same image 
in which case it is easily discarded.

Often images will contain repetitive patterns that are difficult to 
match\footnote{In fact this particular problem has been given attention 
before by for example \cite{fan2011towards}} because the feature points 
covering these patterns will look very similar. If we threshold by 
uniqueness like in Mirror Match this means that these points will often 
be discarded even if they might have good correspondences. To solve this 
problem the enhanced Mirror Match with Clustering makes it possible to 
look at groups of similar feature points one at a time and within each 
group finding the best matches. This is done by taking the set of 
feature points from both images and clustering. The resulting partitions 
will each contain feature points that are similar to each other. In some 
cases the clustering will end up grouping feature points of only one 
image together which can then easily be discarded in the matching 
process. In other cases partitions will contain feature points from both 
images that can then be matched using \emph{MM} but with lower 
thresholds.

The report is organized as follows. In section \ref{algorithms} I will 
describe the two algorithms. Section \ref{experiment} will cover the 
experimental set up and introduce the image set that is used in testing.  
In section \ref{results} I will go through the results and conclude.

\section{Algorithms}
\label{algorithms}

\begin{algorithm}
\caption{Mirror Match Algorithm}
\label{alg-mm}
\begin{algorithmic}
\Require $images$ : set of images, $t$ : float $\in \mathbb{R}$
\State $matches_{init}\gets \varnothing$
\State $matches_{final}\gets \varnothing$
\State $features\gets \varnothing$
\ForAll{$I_i \in images$} \Comment Gather features of all images
	\State $features\gets features \cup getFeatures(I_i)$
\EndFor
\ForAll{$f_i \in features$} \Comment Get best match for all features
	\State $f_m,f_n \gets getTwoNearestNeighbors(features \ {f_i})$
	\State $ratio \gets getSimilarity(f_i, f_n) / getSimilarity(f_i, 
f_m)$
	\If{$ratio < t$}
		\State $matches_{init}\gets \left(f_i, f_m\right)$
	\EndIf
\EndFor
\ForAll{$\left(f_i, f_j \right) \in matches_{init}$} \Comment Filter 
matches
\If{$\left(f_j, f_i \right) \in matches_{init} \wedge getImg(f_i) \neq 
getImg(f_j)$}
		\State $matches_{final} \gets (f_i, f_j)$
		\State $matches_{init} \gets matches_{init} \ (f_j, f_i)$
	\EndIf
\EndFor \\
\Return $matches_{final}$
\end{algorithmic}
\end{algorithm}

In this report I propose two algorithms for better matching feature 
points without any geometric assumptions about the images matched. The 
two algorithms are complementary in the sense that the first, Mirror 
Match (\emph{MM}) is less complex but performs slightly worse than 
Mirror Match with Clustering (\emph{MMC}). In cases where high 


\section{Experiments and Image Set}
\label{experiment}

\section{Results}
\label{results}
\subsection{Graph Clustering}

Given a set of feature points from two images, $im_1$ and $im_2$: $F_k = 
\{k_i, k_j$ for $k_i \in im_1, k_j \in im_2\}$ as well as a matching 
function $M(k_i, k_j) \rightarrow \mathbb{R}$ that takes two feature 
points in an image and return their matching score, we can define a 
matrix $A$ where each element $A_ij = M(k_i, k_j)$. A can be interpreted 
as the \emph{adjecency matrix} of the fully connected graph where each 
vertex corresponds to a keypoint and the edge between two vertices has a 
weight equal to the distance between the two corresponding keypoints.

This representation reduces the problem of partitioning the keypoints 
into groups reduces the problem to that of graph clustering or community 
structure depending on the context. In the literature there are various 
ways of clustering a graph according to different measures of what 
constitutes an optimal partitioning. Traditionally the most used 
clustering algorithms have been K-means and spectral clustering, but in 
recent years a host of new algorithms have been proposed based on both 
Newman's concept of graph modularity\footnote{Introduced in 
\cite{girvan2002}, discussed in \cite{brandes2007} and used in 
\cite{blondel2008} as well as others} as well as information theoretical 
measures\footnote{See for example \cite{rosvall2008}} and the Potts spin 
model from physics\footnote{Used in \cite{ronhovde2009}} just to mention 
a few approaches. Many of the new algorithms differ from K-means 
clustering and Spectral clustering in that they don't require the number 
of expected to clusters to be specified beforehand\footnote{Among the 
aforementioned methods, this is true for \cite{blondel2008} and 
\cite{rosvall2008}}.  Furthermore, on tests done using randomly 
generated graphs with a known partitioning \cite{blondel2008}, 
\cite{rosvall2008} and \cite{ronhovde2009} perform markedly better than 
spectral clustering and K-means\cite{lancichinetti2009}.

The performance of clustering algorithms is a complicated issue, since 
an optimal clustering given the same graph can vary depending on the 
application. Spectral clustering for example will usually return a 
partitioning where each partition is roughly equal in size\footnote{As 
mentioned in \cite{von2007}} while the Louvain 
clustering\cite{blondel2008} might return partitions of very uneven 
size, even if the modularity measurement has been shown to penalize very 
small clusters\cite{brandes2007}. Both behaviours can be beneficial 
depending on the application, but when clustering feature points, 
maintaining clusters of an even size usually means that some clusters 
will be '\emph{catch-all}' clusters where the feature points that don't 
fit anywhere else are grouped together. The necessity of specifying the 
amount of partitions in for example Spectral clustering or Pott's model 
clustering further exacerbates the issue since smaller partitions are 
then combined into one to achieve the right amount of partitions.

Based on these observations I've decided to use Louvain clustering for 
grouping feature points since it's fast, performs well, doesn't require 
parameters and doesn't emphasize partitions of equal sizes.

\subsection{Clustering feature points using Louvain clustering}

\begin{algorithm}
\caption{Cluster-Match Algorithm}
\label{alg-simple}
\begin{algorithmic}
\Require $images$ : set of images, $n$ : int $\in \mathbb{N}$
\State $matches\gets \varnothing$
\ForAll{$I_i \in images$} \Comment Gather features of all images
	\State $f_i\gets getFeatures(I_i)$
\EndFor
\State $A\gets getAdjecencyMatrix(f_1, f_2,\; \ldots \;, f_n)$
\State $A_{norm}\gets 1 - normalize(A)$
\Comment Shorter distance is higher weight
\State $A_{pruned}\gets pruneEdges(A_{norm},\alpha)$
\State $P\gets cluster(A_{pruned})$ 
\Comment P is a set of partitions
\ForAll{$p \in P$} \Comment p is a set of feature points
	\State $trait\gets \left\{p \cap f_i \mid p \cap f_i \neq 
\varnothing, f_i \in \left\{f_1, f_2,\; \ldots \;, f_n\right\}\right\}$
	\If{$\left\vert trait \right\vert > 1$}
		\State $matches\gets getPositions(trait)$
		\Comment Position of trait in each image
	\EndIf
\EndFor \\
\Return matches
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg-simple} shows the basic steps taken to find matches 
in two or more images. This algorithm is a simplified version of the 
proposed method completely agnostic of the geometry of the feature 
points in the picture. To better explain the algorithm here is a step by 
step explication of the inputs and functions mentioned:
\begin{itemize}
\item[]{\emph{images}: The images we are matching}
\item[]{\emph{$n$}: The number of weights per row in the adjecency 
matrix.}
\item[]{\emph{getFeatures}: Returns a set of feature points given an 
image. In practice the BRIEF\cite{calonder2010brief} feature point 
detector and descriptor has worked well, but it could easily be replaced 
with any other feature point type.} 
% TODO: include a comparison of SURF, SIFT, etc
\item[]{\emph{getAdjecencyMatrix}: Based on the distance measure used to 
compare the similarity of the feature points a symmetric adjecency 
matrix is returned.}
\item[]{\emph{pruneEdges}: The result of the Louvain clustering depends 
on the density of the graph. To ensure an optimal number of partitions 
we only keep the $n$ highest weights per row and column in the adjecency 
matrix, making sure it's still symmetric afterwards. In practice setting 
$n$ to around four or five yields good results.}
\item[]{\emph{cluster}: Cluster the graph corresponding to the adjecency 
matrix given using the Louvain algorithm in \cite{blondel2008}.}
\item[]{\emph{getPositions}: Given a trait consisting of features in 
several different images, find the positions of traits in each image and 
return them. This could either be done by taking a mean of the feature 
points in an image, or by returning the position of the highest matching 
feature points within the trait. In practice ... TODO}
\end{itemize}

\subsubsection{On the subject of features}

TODO: Show examples of the same image with feature points from different 
detectors and explain the difference. Go in to why brief seems to work 
best in practice and superficially explain how brief works compared with 
SIFT. 



\begin{algorithm}
\caption{Cluster-Match Algorithm With Geometry}
\label{alg-geometry}
\begin{algorithmic}
\Require $images$ : set of images, $\alpha$ : float $\in \left[0, 
1\right]$, $\beta$ : float $\in \left[0, 1\right]$
\State $matches\gets \varnothing$
\ForAll{$I_i \in images$} \Comment Gather features of all images
	\State $f_i\gets getFeatures(I_i)$
\EndFor
\State $A\gets getAdjecencyMatrix(f_1, f_2,\; \ldots \;, f_n)$
\State $B\gets getGeomDistanceMatrix(f_1, f_2,\; \ldots \;, f_n)$
\State $M\gets getInterImageMask(f_1, f_2,\; \ldots \;, f_n)$
\State $B_{norm}\gets 1 - normalize(G)$ \State $A_{norm}\gets 1 - 
normalize(A)$
\State $A_{geom}\gets A_{norm}\left[ \neg M\right] + \beta \cdot 
B_{norm}\left[M\right]$
\State $A_{pruned}\gets pruneEdges(A_{geom},\alpha)$
\State $P\gets cluster(A_{pruned})$ 
\ForAll{$p \in P$}
	\State $trait\gets \left\{p \cap f_i \mid p \cap f_i \neq 
\varnothing, f_i \in \left\{f_1, f_2,\; \ldots \;, f_n\right\}\right\}$
	\If{$\left\vert trait \right\vert > 1$}
		\State $matches\gets getPositions(trait)$
		\Comment Position of trait in each image
	\EndIf
\EndFor \\
\Return matches
\end{algorithmic}
\end{algorithm}

While clustered feature points are often geometrically close together, 
there might be cases where several points in the same image are similar 
enough to all be included in one cluster despite their geometrical 
distance.  To solve this problem we can either modify the clustering 
algorithm to take the geometric distances into account like 
\cite{das2008event}.  Alternatively we can modify the weight matrix to 
penalize putting distanced feature points in the same cluster. However 
we only know the geometrical distance between two feature points as long 
as both belong to the same image. One way to approach this problem is to 
replace all the weights of edges between two feature points within the 
same image with their distance. For the case of two images the 
normalized adjecency matrix is defined as follows:
%
$$A_{norm}=
\begin{bmatrix}
w_{1,1}   & w_{1,2}   & \ldots & w_{1,n-1}   & w_{1,n}     \\
w_{2,1}   & w_{2,2}   & \ldots & w_{2,n-1}   & w_{2,n}     \\
\vdots    & \vdots    & \ddots & \vdots      & \vdots      \\
w_{n-1,1} & w_{n-1,2} & \ldots & w_{n-1,n-1} & w_{n-1,n}   \\
w_{n,1}   & w_{n,2}   & \ldots & w_{n,n-1}   & w_{n,n}
\end{bmatrix}=
\begin{bmatrix}
A_{im1} & S       \\
T       & A_{im2} \\
\end{bmatrix}
$$
%
In a similar manner we can define an adjecency matrix based on the 
geometrical distance. Note for example how $d_{1,n}$ can take any value 
as it denotes the geometric distance of feature points in two different 
images and as such can be set to any value.
%
$$B_{norm}=
\begin{bmatrix}
d_{1,1}   & d_{1,2}   & \ldots & d_{1,n-1}   & d_{1,n}     \\
d_{2,1}   & d_{2,2}   & \ldots & d_{2,n-1}   & d_{2,n}     \\
\vdots    & \vdots    & \ddots & \vdots      & \vdots      \\
d_{n-1,1} & d_{n-1,2} & \ldots & d_{n-1,n-1} & d_{n-1,n}   \\
d_{n,1}   & d_{n,2}   & \ldots & d_{n,n-1}   & d_{n,n}
\end{bmatrix}=
\begin{bmatrix}
B_{im1} & R       \\
Q       & B_{im2} \\
\end{bmatrix}
$$
%
Given these two and a weight factor $\beta$ we can construct a new 
weight matrix as follows:
%
$$C=
\begin{bmatrix}
\beta \cdot B_{im1} & S       \\
T       & \beta \cdot B_{im2} \\
\end{bmatrix}
$$
%
Since we prune the matrix afterwards, it's important to choose a weight 
factor that doesn't lead to all edges being either intra image edges or 
intra image edges, since the first would create no clusters across 
images and the latter would mean that the geometrical information would 
be discarded and the bipartite graph constituting of edges between 
feature points in the two images would be used instead.

\bibliographystyle{abbrv}
\bibliography{bibliography}
\end{document}

